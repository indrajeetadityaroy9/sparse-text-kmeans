\section{Experiments}
\label{sec:experiments}
We now show that the cross-polytope LSH, combined with our multiprobe extension, leads to an algorithm that is also efficient in practice and improves over the hyperplane LSH on several data sets.
The focus of our experiments is the query time for an \emph{exact} nearest neighbor search.
Since hyperplane LSH has been compared to other nearest-neighbor algorithms before \cite{SSL14}, we limit our attention to the relative speed-up compared with hyperplane hashing.


We evaluate the two hashing schemes on three types of data sets.
We use a synthetic data set of randomly generated points because this allows us to vary a single problem parameter while keeping the remaining parameters constant.
We also investigate the performance of our algorithm on real data: two tf-idf data sets \cite{Lichman2013} and a set of SIFT feature vectors \cite{JDS11}.
We have chosen these data sets in order to illustrate when the cross-polytope LSH gives large improvements over the hyperplane LSH, and when the improvements are more modest.
See Appendix \ref{app:experiments} for a more detailed description of the data sets and our experimental setup (implementation details, CPU, etc.).

In all experiments, we set the algorithm parameters so that the empirical probability of successfully finding the exact nearest neighbor is at least 0.9.
Moreover, we set the number of LSH tables $L$ so that the amount of additional memory occupied by the LSH data structure is comparable to the amount of memory necessary for storing the data set.
We believe that this is the most interesting regime because significant memory overheads are often impossible for large data sets.
In order to determine the parameters that are not fixed by the above constraints,
we perform a grid search over the remaining parameter space and report the best combination of parameters.
For the cross-polytope hash, we consider ``partial'' cross-polytopes in the last of the $k$ hash functions in order to get a smooth trade-off between the various parameters (see Section \ref{sec:partial_cp}).

\paragraph{Multiprobe experiments.} 
In order to demonstrate that the multiprobe scheme is critical for making the cross-polytope LSH competitive with hyperplane hashing, we compare the performance of a ``standard'' cross-polytope LSH data structure with our multiprobe variant on an instance of the random data set ($n=2^{20}$, $d=128$).
As can be seen in Table \ref{table:multiprobe} (Appendix \ref{app:experiments}), the multiprobe variant is about $13\times$ faster in our memory-constrained setting ($L = 10$).
Note that in all of the following experiments, the speed-up of the multiprobe cross-polytope LSH compared to the multiprobe hyperplane LSH is less than $11 \times$.
Hence without our multiprobe addition, the cross-polytope LSH would be slower than the hyperplane LSH, for which a multiprobe scheme is already known \cite{lv2007multi}.

\paragraph{Experiments on random data.} 
Next, we show that the better time complexity of the cross-polytope LSH already applies for moderate values of $n$.
In particular, we compare the cross-polytope LSH, combined with fast rotations (Section \ref{sec:cppractical}) and our multiprobe scheme, to a multi-probe hyperplane LSH on random data.
We keep the dimension $d = 128$ and the distance to the nearest neighbor $R = \sqrt{2}/2$ fixed, and vary the size of the data set from $2^{20}$ to $2^{28}$.
The number of hash tables $L$ is set to $10$.
For $2^{20}$ points, the cross-polytope LSH is already $3.5 \times$ faster than the hyperplane LSH, and for $n= 2^{28}$ the speedup is $10.3\times$ (see Table \ref{table:randomdata} in Appendix \ref{app:experiments}).
Compared to a linear scan, the speed-up achieved by the cross-polytope LSH ranges from $76\times$ for $n=2^{20}$ to about $700\times$ for $n=2^{28}$.


\paragraph{Experiments on real data.} On the SIFT data set ($n = 10^6$ and $d = 128$), the cross-polytope LSH achieves a modest speed-up of $1.2\times$ compared to the hyperplane LSH (see Table \ref{table:realdata}).
On the other hand, the speed-up is is $3 - 4\times$ on the two tf-idf data sets, which is a significant improvement considering the relatively small size of the NYT data set ($n \approx 300,000$).
One important difference between the data sets is that the typical distance to the nearest neighbor is smaller in the SIFT data set, which can make the nearest neighbor problem easier (see Appendix \ref{app:experiments}).
Since the tf-idf data sets are very high-dimensional but sparse ($d \approx 100,000$), we use the feature hashing approach described in Section \ref{sec:cppractical} in order to reduce the hashing time of the cross-polytope LSH (the standard hyperplane LSH already runs in time proportional to the sparsity of a vector).
We use $512$ and $2048$ as feature hashing dimensions for NYT and pubmed, respectively.

\begin{table}
\centering
\begin{tabular}{ccccccccc}
\toprule
Data set & Method & \specialcell{Query\\time (ms)} & \specialcell{\textbf{Speed-up}\\\textbf{vs HP}} & Best $k$ & \specialcell{Number of\\candidates} & \specialcell{Hashing\\time (ms)} & \specialcell{Distances\\time (ms)} \\
\midrule
NYT & HP & 120 ms & & 19 & 57,200 & 16 & 96 \\
NYT & CP & 35 ms & \mbox{\boldmath$3.4\times$} & 2 (64) & 17,900 & 3.0 & 30 \\
\midrule
pubmed & HP & 857 ms & & 20 & 1,480,000 & 36 & 762 \\
pubmed & CP & 213 ms & \mbox{\boldmath$4.0\times$} & 2 (512) & 304,000 & 18 & 168 \\
\midrule
SIFT & HP & 3.7 ms & & 30 & 18,628 & 0.2 & 3.0 \\
SIFT & CP & 3.1 ms & \mbox{\boldmath$1.2\times$} & 6 (1) & 13,000 & 0.6 & 2.2 \\
\bottomrule
\end{tabular}
\caption{Average running times for a single nearest neighbor query with the hyperplane (HP) and cross-polytope (CP) algorithms on three real data sets.
The cross-polytope LSH is faster than the hyperplane LSH on all data sets, with significant speed-ups for the two tf-idf data sets NYT and pubmed.
For the cross-polytope LSH, the entries for $k$ include both the number of individual hash functions per table and (in parenthesis) the dimension of the last of the $k$ cross-polytopes.}
\label{table:realdata}
\end{table}
