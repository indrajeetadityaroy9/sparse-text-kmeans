\section{Further description of experiments}
\label{app:experiments}
In order to compare meaningful running time numbers, we have written fast C++ implementations of both the cross-polytope LSH and the hyperplane LSH.
This enables a fair comparison since both implementations have been optimized by us to the same degree.
In particular, hyperplane hashing can be implemented efficiently using a matrix-vector multiplication sub-routine for which we use the eigen library (eigen is also used for all other linear algebra operations).
For the fast pseudo-random rotation in the cross-polytope LSH, we have written a SIMD-optimized version of the Fast Hadamard Transform (FHT).
We compiled our code with g++ 4.9 and the -O3 flag.
All experiments except those in Table \ref{table:randomdata} ran on an Intel Core i5-2500 CPU (3.3 - 3.7 GHz, 6 MB cache) with 8 GB of RAM.
Since 8 GB of RAM was too small for the larger values of $n$, we ran the experiments in Table \ref{table:randomdata} on a machine with an Intel Xeon E5-2690 v2 CPU (3.0 GHz, 25 MB cache) and 512 GB of RAM.

\colorlet{fillblue}{blue!35!white}
\newlength{\plothorizspacing}
\setlength{\plothorizspacing}{2.5cm}
\newlength{\plotvertspacing}
\setlength{\plotvertspacing}{-2.5cm}

\pgfplotsset{distanceplot/.style={%
  scale only axis,
  enlarge x limits=false,
  enlarge y limits=false,
  ylabel shift=-5pt,
  title style={yshift=-5pt},
  grid=none,
  no markers,
  every axis plot/.append style={line width=1pt},
  ymin=0,
  ymax=1,
  ytick={0,0.5,1},
  xmin=0,
  xmax=1.414,
  xtick={0,0.354,0.707,1.061,1.413},
  xticklabels={$0$,$\frac{\sqrt{2}}{4}$,$\frac{\sqrt{2}}{2}$,$\frac{3\sqrt{2}}{4}$,$\sqrt{2}$},
  xlabel={Distance to nearest neighbor},
  ylabel={Fraction of points},
  width=4cm,
  height=3cm,
  xtick pos=left,
  xtick align=outside,
}}


\begin{figure}[htb]
\centering
\begin{tikzpicture}
\begin{axis}[name=randomplot,distanceplot,title={Random data set}]
\addplot+[ybar interval,fill=fillblue] table[x=x,y=y] {plot_data/distance_histogram_random1m.txt};
\end{axis}
\begin{axis}[name=nytplot,distanceplot,anchor=south west,at=(randomplot.south east),xshift=\plothorizspacing,title={NYT data set}]
\addplot+[ybar interval,fill=fillblue] table[x=x,y=y] {plot_data/distance_histogram_nyt.txt};
\end{axis}
\begin{axis}[name=siftplot,distanceplot,anchor=north west,at=(randomplot.south west),yshift=\plotvertspacing,title={SIFT data set}]
\addplot+[ybar interval,fill=fillblue] table[x=x,y=y] {plot_data/distance_histogram_sift1m.txt};
\end{axis}
\begin{axis}[name=pubmedplot,distanceplot,anchor=north west,at=(nytplot.south west),yshift=\plotvertspacing,title={pubmed data set}]
\addplot+[ybar interval,fill=fillblue] table[x=x,y=y] {plot_data/distance_histogram_pubmed.txt};
\end{axis}
\end{tikzpicture}
\caption{Distance to the nearest neighbor for the four data sets used in our experiments.
The SIFT data set has the closest nearest neighbors.}
\label{fig:datasetsdistances}
\end{figure}


In our experiments, we evaluate the performance of the cross-polytope LSH on the following data sets.
Figure \ref{fig:datasetsdistances} shows the distribution of distances to the nearest neighbor for the four data sets.
\begin{description}[leftmargin=.5cm]
\item[random] For the random data sets, we generate a set of $n$ points uniformly at random on the unit sphere.
In order to generate a query, we pick a random point $q'$ from the data set and generate a point at distance $R$ from $q'$ on the unit sphere.
In our experiments, we vary the dimension of the point set between 128 and 1,024.
Experiments with the random data set are useful because we can study the impact of various parameters (e.g., the dimension $d$ or the number of points $n$) while keeping the remaining parameters constant.
\item[pubmed / NYT] The pubmed and NYT data sets contain bag-of-words representations of medical paper abstracts and newspaper articles, respectively \cite{Lichman2013}.
We convert this representation into standard tf-idf feature vectors with dimensionality about 100,000.
The number of points in the pubmed data set is about 8 million, for NYT it is 300,000.
Before setting up the LSH data structures, we set 1000 data points aside as query vectors.
When selecting query vectors, we limit our attention to points for which the inner product with the nearest neighbor is between 0.3 and 0.8.
We believe that this is the most interesting range since near-duplicates (inner product close to 1) can be identified more efficiently with other methods, and points without a close nearest neighbor (inner product less than 0.3) often do not have a semantically meaningful match.
\item[SIFT] We use the standard data set of one million SIFT feature vectors from \cite{JDS11}, which also contains a set of 10,000 query vectors.
The SIFT feature vectors have dimension $128$ and (approximately) live on a sphere.
We normalize the feature vectors to unit length but keep the original nearest neighbor assignments---this is possible because only a very small fraction of nearest neighbors changes through normalization.
We include this data set as an example where the speed-up of the cross-polytope LSH is more modest.
\end{description}

\begin{table}[htb]
\centering
\begin{tabular}{cccccccc}
\toprule
Method & $k$ & \specialcell{Last CP\\dimension} & \specialcell{Extra\\probes} & \specialcell{\textbf{Query}\\\textbf{time (ms)}} & \specialcell{Number of\\candidates} & \specialcell{CP hashing \\time (ms)} & \specialcell{Distances\\time (ms)} \\
\midrule
Single-probe & 1 & 128 & 0 & \textbf{6.7} & 39800 & 0.01 & 6.3 \\
Multiprobe & 3 & 16 & 896 & \textbf{0.51} & 867 & 0.22 & 0.16 \\
\bottomrule
\end{tabular}
\caption{Comparison of ``standard'' LSH using the cross-polytope (CP) hash vs.\ our multiprobe variant ($L = 10$ in both cases).
On a random data set with $n = 2^{20}$, $d = 128$, and $R = \sqrt{2} / 2$, the single-probe scheme requires $13\times$ more time per query.
Due to the larger value of $k$, the multiprobe variant performs fewer distance computations, which leads to a better trade-off between the hash computation time and the time spent on computing distances to candidates from the hash tables.}
\label{table:multiprobe}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{cccccc}
\toprule
Data set size $n$ & $2^{20}$ & $2^{22}$ & $2^{24}$ & $2^{26}$ & $2^{28}$ \\
\midrule
HP query time (ms) & 2.6 & 7.4 & 25 & 63 & 185 \\
CP query time (ms) & 0.75 & 1.4 & 3.1 & 8.8 & 18 \\
\textbf{Speed-up} & \mbox{\boldmath$3.5\times$} & \mbox{\boldmath$5.3\times$} & \mbox{\boldmath$8.1\times$} & \mbox{\boldmath$7.2\times$} & \mbox{\boldmath$10.3\times$} \\
$k$ for CP & 3 (16) & 3 (64) & 3 (128) & 4 (2) & 4 (64) \\
\bottomrule
\end{tabular}
\caption{Average running times for a single nearest neighbor query with the hyperplane (HP) and cross-polytope (CP) algorithms on a random data set with $d = 128$ and $R = \sqrt{2}/2$.
The cross-polytope LSH is up to $10\times$ faster than the hyperplane LSH.
The last row of the table indicates the optimal choice of $k$ for the cross-polytope LSH and (in parenthesis) the dimension of the last of the $k$ cross-polytopes; all other cross-polytopes have full dimension 128.
Note that the speed-up ratio is not monotonically increasing because the cross-polytope LSH performs better for values of $n$ where the optimal setting of $k$ uses a last cross-polytope with high dimension.
}
\label{table:randomdata}
\end{table}
