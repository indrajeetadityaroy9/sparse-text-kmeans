\section{Introduction}
Nearest neighbor search is a key algorithmic problem with applications in several fields including computer vision, information retrieval, and machine learning~\cite{nnlv}.
Given a set of $n$ points $P \subset \R^d$, the goal is to build a data structure that answers nearest neighbor queries efficiently: for a given query point $q \in \R^d$, find the point $p \in P$ that is closest to $q$ under an appropriately chosen distance metric. 
The main algorithmic design goals are usually a fast query time, a small memory footprint, and---in the approximate setting---a good quality of the returned solution.

There is a wide range of algorithms for nearest neighbor search based
on techniques such as space partitioning with indexing, as well as
dimension reduction or sketching~\cite{samet2006foundations}.
A popular method for point sets in high-dimensional spaces is Locality-Sensitive Hashing (LSH)~\cite{HIM12,Cha02}, an approach that offers a \emph{provably} sub-linear query time and sub-quadratic space complexity, and
has been shown to achieve  good empirical performance in a variety of applications~\cite{nnlv}.
The method relies on the notion of \emph{locality-sensitive hash functions}.
Intuitively, a hash function is locality-sensitive if its probability of collision is higher for ``nearby'' points than for points that are ``far apart''.
More formally, two points are nearby if their distance is at most
$r_1$, and they are far apart if their distance is at
least~$r_2=c \cdot r_1$, where $c > 1$ quantifies the gap between ``near'' and ``far''.
The quality of a hash function is characterized by two key parameters: $p_1$ is the collision probability for nearby points, and~$p_2$ is the collision probability for points that are far apart.
The gap between $p_1$ and $p_2$ determines how ``sensitive'' the hash function is to changes in distance, and this property is captured by the parameter $\rho=\frac{\log 1 / p_1}{\log 1 / p_2}$, which can usually be expressed as a function of the distance gap $c$. 
The problem of designing good locality-sensitive hash functions and LSH-based efficient nearest neighbor search algorithms has attracted significant attention over the last few years.

In this paper, we focus on LSH for the Euclidean distance \emph{on the unit sphere}, which is an important special case for several reasons.
First, the spherical case is relevant in practice: Euclidean distance on a sphere corresponds to the \emph{angular distance} or \emph{cosine similarity}, which are commonly used in applications such as comparing image feature vectors \cite{JDS11}, speaker representations \cite{SSL14}, and tf-idf data sets \cite{sundaram2013streaming}.
Moreover, on the theoretical side, the paper \cite{AR15} shows a reduction from Nearest Neighbor Search in the \emph{entire} Euclidean space to the spherical case.
These connections lead to a natural question: what are good LSH families for this special case?

On the theoretical side, the recent work of \cite{AINR-subLSH, AR15} gives the best known provable guarantees for LSH-based nearest neighbor search w.r.t.\ the Euclidean distance on the unit sphere.
Specifically, their algorithm has a query time of $O(n^\rho)$ and
space complexity of $O(n^{1 +  \rho})$ for $\rho= \frac{1}{2 c^2 -
  1}.$\footnote{This running time is known to be
  essentially optimal for a large class of algorithms~\cite{Dubiner, ar15lower}.}
E.g., for the approximation factor $c=2$, the algorithm achieves a query time of $n^{1/7+o(1)}$.
At the heart of the algorithm is an LSH scheme called {\em Spherical LSH}, which works for unit vectors. 
Its key property is that it can distinguish between distances $r_1= \sqrt{2}/c$ and $r_2=\sqrt{2}$ with probabilities yielding $\rho =  \frac{1}{2 c^2 -1}$ (the formula for the full range of distances is more complex and given in Section~\ref{sec:crossPoly}). 
Unfortunately, the scheme as described in the paper is not applicable in practice as it is based on rather complex hash functions that are very time consuming to evaluate. 
E.g., simply evaluating a~single hash function from \cite{AR15} can take more time than a linear scan over $10^6$ points.
Since an LSH data structure contains many individual hash functions, using their scheme would be slower than a~simple linear scan over all points in $P$ unless the number of points $n$ is extremely large.


On the practical side, the hyperplane LSH introduced in the influential work of Charikar \cite{Cha02} has worse theoretical guarantees, but works well in practice.
Since the hyperplane LSH can be implemented very efficiently, it is the standard hash function in practical LSH-based nearest neighbor algorithms\footnote{Note that if the data points are {\em binary}, more efficient LSH schemes exist~\cite{shrivastava2012fast,  shrivastava2014densifying}. However, in this paper we consider algorithms for general (non-binary) vectors.}  and the resulting implementations has been shown to improve over a linear scan on real data by multiple orders of magnitude~\cite{lv2007multi,sundaram2013streaming}.    

The aforementioned discrepancy between the theory and practice of LSH raises an important question: is there a locality-sensitive hash function with {\em optimal} guarantees that also improves over the hyperplane LSH in practice?

In this paper we show that there is a family of locality-sensitive hash functions that achieves both objectives.
Specifically, the hash functions match the theoretical guarantee of Spherical LSH from \cite{AR15} and, when combined with additional techniques, give better experimental results than the hyperplane LSH.
More specifically, our contributions are:

\paragraph{Theoretical guarantees for the cross-polytope LSH.} We show that a hash function based on randomly rotated cross-polytopes (i.e., unit balls of the $\ell_1$-norm) achieves the same parameter $\rho$ as the Spherical LSH scheme in \cite{AR15}, assuming data points are unit vectors.
While the cross-polytope LSH family has been proposed by researchers before~\cite{terasawa2007spherical,eshghi2008locality} we give the first theoretical analysis of its performance.  

\paragraph{Fine-grained lower bound for cosine similarity LSH.} 
To highlight the difficulty of obtaining optimal {\em and} practical LSH schemes,
we prove the first {\em non-asymptotic} lower bound on the trade-off
between the collision probabilities $p_1$ and $p_2$.
So far, the optimal LSH upper bound $\rho=\tfrac{1}{2c^2-1}$ (from
\cite{AINR-subLSH, AR15} and cross-polytope from here) attain this bound only in
the limit, as $p_1,p_2\to 0$. Very small~$p_1$ and~$p_2$ are undesirable
since the hash evaluation time is often proportional to $1/p_2$. Our
lower bound proves this is unavoidable: if we require $p_2$ to be
large, $\rho$ has to be suboptimal.

This result has two important implications for designing practical
hash functions. First, it shows that the trade-offs achieved by the
cross-polytope LSH and the scheme of \cite{AINR-subLSH, AR15} are essentially
optimal.  Second, the lower bound guides design of future LSH
functions: if one is to significantly improve upon the cross-polytope
LSH, one has to design a hash function that is computed more efficiently than by explicitly
enumerating its range (see Section~\ref{sec_lower} for a more detailed
discussion).

\paragraph{Multiprobe scheme for the cross-polytope LSH.} The space
complexity of an LSH data structure is sub-\emph{quadratic}, but even
this is often too large (i.e., strongly super-\emph{linear} in the
number of points), and several methods have been proposed to address
this issue.  Empirically, the most efficient scheme is
multiprobe LSH~\cite{lv2007multi}, which leads to a significantly reduced memory
footprint for the hyperplane LSH.  In order to make the cross-polytope
LSH competitive in practice with the multiprobe hyperplane LSH, we propose a novel
multiprobe scheme for the cross-polytope LSH.  

We complement these contributions with an experimental evaluation on both real and synthetic data (SIFT vectors, tf-idf data, and a random point set). In order to make the cross-polytope LSH practical, we combine it with fast pseudo-random rotations~\cite{AC09} via
the Fast Hadamard Transform, and feature hashing~\cite{WDLSA09} to exploit sparsity of data.
Our results show that for data sets with around $10^5$ to~$10^8$ points, our multiprobe variant of the cross-polytope LSH is up to $10\times$ faster than an efficient implementation of the hyperplane LSH, and up to $700\times$ faster than a linear scan.
To the best of our knowledge, our combination of techniques provides the first ``exponent-optimal'' algorithm that empirically improves over the hyperplane LSH in terms of query time for an \emph{exact} nearest neighbor search.
